{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1286PXoSV5Oo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fish: Unknown command: nvidia-smi\n",
            "fish: \n",
            "nvidia-smi\n",
            "^~~~~~~~~^\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check and enable GPU\n",
        "!nvidia-smi\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation and Loading\n",
        "\n",
        "We prepare the MNIST dataset using torchvision, applying a simple transformation to convert images to tensors. The training data is loaded using a DataLoader for efficient batching and shuffling. This setup is essential for training the Conditional Variational Autoencoder (CVAE) model on handwritten digit images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hrzNMBakXd2u"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST normalization\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "We define a Conditional Variational Autoencoder (CVAE) model using PyTorch. The encoder takes both the input image and its corresponding label (as an additional channel), processes them through convolutional layers, and outputs the mean and log-variance for the latent space. The decoder reconstructs the image from the latent vector concatenated with the label information. This architecture enables the model to generate images conditioned on specific digit labels, allowing for controlled generation of handwritten digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Qv7j4SH2Xefj"
      },
      "outputs": [],
      "source": [
        "def improved_loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
        "    # Use MSE loss for better reconstruction\n",
        "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    \n",
        "    # KL divergence with beta weighting\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return MSE + beta * KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZWK6vyBTYRpd"
      },
      "outputs": [],
      "source": [
        "class ImprovedCVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(ImprovedCVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.label_dim = 10\n",
        "\n",
        "        # Improved Encoder with BatchNorm and Dropout\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1 + 1, 32, kernel_size=4, stride=2, padding=1),  # 28x28 -> 14x14\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),     # 14x14 -> 7x7\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),    # 7x7 -> 4x4 (with padding adjustment)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        \n",
        "        # Correct flattened size calculation: \n",
        "        # Input: 28x28 -> Conv1: 14x14 -> Conv2: 7x7 -> Conv3: 3x3 (28->14->7->3)\n",
        "        # But with padding=1: 28->14->7->4, so it's actually 128 * 3 * 3 = 1152\n",
        "        self.flattened_size = 128 * 3 * 3  # Fixed calculation\n",
        "\n",
        "        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)\n",
        "\n",
        "        # Improved Decoder\n",
        "        self.decoder_input = nn.Linear(latent_dim + self.label_dim, 128 * 3 * 3)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (128, 3, 3)),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 3x3 -> 6x6\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # 6x6 -> 12x12\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=5, stride=2, padding=1),    # 12x12 -> 25x25\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(1, 1, kernel_size=4, stride=1, padding=0),     # 25x25 -> 28x28\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x, labels_onehot):\n",
        "        # Better label conditioning - create label map with same spatial dimensions as input\n",
        "        label_map = labels_onehot.view(-1, 10, 1, 1).expand(-1, -1, 28, 28)\n",
        "        # Use first channel of label map (or sum across label dimensions)\n",
        "        label_channel = torch.sum(label_map * torch.arange(10, device=x.device).view(1, 10, 1, 1), dim=1, keepdim=True) / 45.0  # normalize by max sum\n",
        "        x_cond = torch.cat([x, label_channel], dim=1)\n",
        "\n",
        "        encoded = self.encoder(x_cond)\n",
        "        mu = self.fc_mu(encoded)\n",
        "        logvar = self.fc_logvar(encoded)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        z_cond = torch.cat([z, labels_onehot], dim=1)\n",
        "        dec_input = self.decoder_input(z_cond)\n",
        "        x_recon = self.decoder(dec_input)\n",
        "        return x_recon, mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "\n",
        "We train the Conditional Variational Autoencoder (CVAE) using the MNIST training dataset. For each epoch, the model processes batches of images and their corresponding labels, computes the reconstruction and KL divergence losses, and updates the model parameters using the Adam optimizer. The training loop prints the average loss per epoch, providing insight into the model's learning progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_AWsUR2XxYC",
        "outputId": "d3ee14f7-0a63-4fd5-ae41-75604093fd3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 171.1948, Val Loss: 141.3906\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 2/50\n",
            "Train Loss: 100.5693, Val Loss: 47.9909\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 3/50\n",
            "Train Loss: 46.4329, Val Loss: 45.4971\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 4/50\n",
            "Train Loss: 44.2546, Val Loss: 43.6108\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 5/50\n",
            "Train Loss: 42.2005, Val Loss: 41.4392\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 6/50\n",
            "Train Loss: 40.1443, Val Loss: 39.3317\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 7/50\n",
            "Train Loss: 38.1524, Val Loss: 37.2086\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 8/50\n",
            "Train Loss: 36.2111, Val Loss: 35.1986\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 9/50\n",
            "Train Loss: 34.3728, Val Loss: 33.3684\n",
            "LR: 0.001000\n",
            "--------------------------------------------------\n",
            "Epoch 10/50\n",
            "Train Loss: 32.7854, Val Loss: 31.6350\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 11/50\n",
            "Train Loss: 31.3305, Val Loss: 30.5521\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 12/50\n",
            "Train Loss: 30.5530, Val Loss: 29.7114\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 13/50\n",
            "Train Loss: 29.8410, Val Loss: 29.0830\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 14/50\n",
            "Train Loss: 29.2147, Val Loss: 28.5139\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 15/50\n",
            "Train Loss: 28.6664, Val Loss: 27.9456\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 16/50\n",
            "Train Loss: 28.1705, Val Loss: 27.4103\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 17/50\n",
            "Train Loss: 27.7238, Val Loss: 27.1722\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 18/50\n",
            "Train Loss: 27.4231, Val Loss: 26.7415\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 19/50\n",
            "Train Loss: 27.1253, Val Loss: 26.3187\n",
            "LR: 0.000500\n",
            "--------------------------------------------------\n",
            "Epoch 20/50\n",
            "Train Loss: 26.9079, Val Loss: 26.2182\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 21/50\n",
            "Train Loss: 26.5079, Val Loss: 25.8369\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 22/50\n",
            "Train Loss: 26.3404, Val Loss: 25.7475\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 23/50\n",
            "Train Loss: 26.2531, Val Loss: 25.6281\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 24/50\n",
            "Train Loss: 26.1444, Val Loss: 25.4860\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 25/50\n",
            "Train Loss: 26.0573, Val Loss: 25.4214\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 26/50\n",
            "Train Loss: 25.9616, Val Loss: 25.2898\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 27/50\n",
            "Train Loss: 25.8634, Val Loss: 25.1708\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 28/50\n",
            "Train Loss: 25.8078, Val Loss: 25.1503\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 29/50\n",
            "Train Loss: 25.7047, Val Loss: 24.9947\n",
            "LR: 0.000250\n",
            "--------------------------------------------------\n",
            "Epoch 30/50\n",
            "Train Loss: 25.6537, Val Loss: 24.9912\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 31/50\n",
            "Train Loss: 25.4608, Val Loss: 24.7775\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 32/50\n",
            "Train Loss: 25.4164, Val Loss: 24.7435\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 33/50\n",
            "Train Loss: 25.4051, Val Loss: 24.7381\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 34/50\n",
            "Train Loss: 25.3586, Val Loss: 24.7091\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 35/50\n",
            "Train Loss: 25.2963, Val Loss: 24.6913\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 36/50\n",
            "Train Loss: 25.2487, Val Loss: 24.6480\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 37/50\n",
            "Train Loss: 25.2631, Val Loss: 24.5815\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 38/50\n",
            "Train Loss: 25.2292, Val Loss: 24.5874\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 39/50\n",
            "Train Loss: 25.2095, Val Loss: 24.5974\n",
            "LR: 0.000125\n",
            "--------------------------------------------------\n",
            "Epoch 40/50\n",
            "Train Loss: 25.1525, Val Loss: 24.5568\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 41/50\n",
            "Train Loss: 25.1214, Val Loss: 24.4796\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 42/50\n",
            "Train Loss: 25.0700, Val Loss: 24.4148\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 43/50\n",
            "Train Loss: 25.0511, Val Loss: 24.4582\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 44/50\n",
            "Train Loss: 25.0402, Val Loss: 24.3946\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 45/50\n",
            "Train Loss: 25.0254, Val Loss: 24.4080\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 46/50\n",
            "Train Loss: 25.0059, Val Loss: 24.4447\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 47/50\n",
            "Train Loss: 24.9952, Val Loss: 24.4202\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 48/50\n",
            "Train Loss: 24.9954, Val Loss: 24.3979\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 49/50\n",
            "Train Loss: 24.9690, Val Loss: 24.3284\n",
            "LR: 0.000063\n",
            "--------------------------------------------------\n",
            "Epoch 50/50\n",
            "Train Loss: 24.9681, Val Loss: 24.3740\n",
            "LR: 0.000031\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Add validation dataset\n",
        "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "model = ImprovedCVAE(latent_dim=128).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "epochs = 50\n",
        "beta = 1.0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels_onehot = nn.functional.one_hot(labels, num_classes=10).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(images, labels_onehot)\n",
        "        loss = improved_loss_function(recon, images, mu, logvar, beta)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels_onehot = nn.functional.one_hot(labels, num_classes=10).float()\n",
        "            \n",
        "            recon, mu, logvar = model(images, labels_onehot)\n",
        "            loss = improved_loss_function(recon, images, mu, logvar, beta)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"cvae_mnist.pth\")\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mtIyhCV9XyJJ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"cvae_mnist.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EvW-v3l8Yphy"
      },
      "outputs": [],
      "source": [
        "digit = 5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9Hwlz7fPYqUv"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "num_samples = 5\n",
        "label = torch.full((num_samples,), digit, dtype=torch.long)\n",
        "label_onehot = nn.functional.one_hot(label, num_classes=10).float().to(device)\n",
        "\n",
        "# Sample random noise from normal distribution\n",
        "z = torch.randn(num_samples, model.latent_dim).to(device)\n",
        "\n",
        "# Concatenate latent vector with one-hot label\n",
        "z_cond = torch.cat([z, label_onehot], dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8GTxDROHYsDt"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    decoder_input = model.decoder_input(z_cond)\n",
        "    generated = model.decoder(decoder_input) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "ysP9ieE2YukQ",
        "outputId": "745b2d86-8827-4bae-98fb-66800d9ebc4d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generated = generated.cpu()\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    axs[i].imshow(generated[i].squeeze(), cmap='gray')\n",
        "    axs[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
