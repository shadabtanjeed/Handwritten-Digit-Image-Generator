{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqKTN7NHXV81",
        "outputId": "08d5fc4a-6ab9-4989-e9ce-d29fe03836ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jun 21 16:45:48 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check and enable GPU\n",
        "!nvidia-smi\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1286PXoSV5Oo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation and Loading\n",
        "\n",
        "We prepare the MNIST dataset using torchvision, applying a simple transformation to convert images to tensors. The training data is loaded using a DataLoader for efficient batching and shuffling. This setup is essential for training the Conditional Variational Autoencoder (CVAE) model on handwritten digit images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hrzNMBakXd2u"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "We define a Conditional Variational Autoencoder (CVAE) model using PyTorch. The encoder takes both the input image and its corresponding label (as an additional channel), processes them through convolutional layers, and outputs the mean and log-variance for the latent space. The decoder reconstructs the image from the latent vector concatenated with the label information. This architecture enables the model to generate images conditioned on specific digit labels, allowing for controlled generation of handwritten digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qv7j4SH2Xefj"
      },
      "outputs": [],
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWK6vyBTYRpd"
      },
      "outputs": [],
      "source": [
        "class CVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=20):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.label_dim = 10 \n",
        "\n",
        "        # Encoder: takes image + label\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1 + 1, 32, kernel_size=3, stride=2, padding=1),  # 1 (image) + 1 (label as channel)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.flattened_size = 64 * 7 * 7\n",
        "\n",
        "        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)\n",
        "\n",
        "        # Decoder: takes latent vector + label\n",
        "        self.decoder_input = nn.Linear(latent_dim + self.label_dim, 64 * 7 * 7)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (64, 7, 7)),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()  # Output range [0, 1]\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x, labels_onehot):\n",
        "        # Expand label to match image size for conditioning\n",
        "        label_map = labels_onehot.view(-1, 10, 1, 1).repeat(1, 1, 28, 28)  # shape: [batch, 10, 28, 28]\n",
        "        x_cond = torch.cat([x, label_map[:, 0:1, :, :]], dim=1)  # use only 1 channel from label\n",
        "\n",
        "        encoded = self.encoder(x_cond)\n",
        "        mu = self.fc_mu(encoded)\n",
        "        logvar = self.fc_logvar(encoded)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        z_cond = torch.cat([z, labels_onehot], dim=1)\n",
        "        dec_input = self.decoder_input(z_cond)\n",
        "        x_recon = self.decoder(dec_input)\n",
        "        return x_recon, mu, logvar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "\n",
        "We train the Conditional Variational Autoencoder (CVAE) using the MNIST training dataset. For each epoch, the model processes batches of images and their corresponding labels, computes the reconstruction and KL divergence losses, and updates the model parameters using the Adam optimizer. The training loop prints the average loss per epoch, providing insight into the model's learning progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_AWsUR2XxYC",
        "outputId": "d3ee14f7-0a63-4fd5-ae41-75604093fd3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 158.6859\n",
            "Epoch 2, Loss: 115.9956\n",
            "Epoch 3, Loss: 110.1686\n",
            "Epoch 4, Loss: 107.0223\n",
            "Epoch 5, Loss: 105.3051\n",
            "Epoch 6, Loss: 104.1843\n",
            "Epoch 7, Loss: 103.3860\n",
            "Epoch 8, Loss: 102.7382\n",
            "Epoch 9, Loss: 102.2424\n",
            "Epoch 10, Loss: 101.8417\n",
            "Epoch 11, Loss: 101.4121\n",
            "Epoch 12, Loss: 101.0292\n",
            "Epoch 13, Loss: 100.7554\n",
            "Epoch 14, Loss: 100.4662\n",
            "Epoch 15, Loss: 100.2605\n"
          ]
        }
      ],
      "source": [
        "model = CVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels_onehot = nn.functional.one_hot(labels, num_classes=10).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(images, labels_onehot)\n",
        "        loss = loss_function(recon, images, mu, logvar)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader.dataset):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtIyhCV9XyJJ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"cvae_mnist.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvW-v3l8Yphy"
      },
      "outputs": [],
      "source": [
        "digit = 5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hwlz7fPYqUv"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "num_samples = 5\n",
        "label = torch.full((num_samples,), digit, dtype=torch.long)\n",
        "label_onehot = nn.functional.one_hot(label, num_classes=10).float().to(device)\n",
        "\n",
        "# Sample random noise from normal distribution\n",
        "z = torch.randn(num_samples, model.latent_dim).to(device)\n",
        "\n",
        "# Concatenate latent vector with one-hot label\n",
        "z_cond = torch.cat([z, label_onehot], dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GTxDROHYsDt"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    decoder_input = model.decoder_input(z_cond)\n",
        "    generated = model.decoder(decoder_input) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "ysP9ieE2YukQ",
        "outputId": "745b2d86-8827-4bae-98fb-66800d9ebc4d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9IAAAC+CAYAAADZTTdiAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFy1JREFUeJzt3VmM1Wf5wPEfOzPMsG9DWVqB0oBA04JAKtZugSKktYmxmlSbmrhFYzTxChPT2JuaGBOXG6pi02hbSDRUqZWYKNZSAklpy9IqOwMMOzMwwwDD8r9qmn/6PGNfMkJhPp/LL8w5Bzi/c87DSZ63x+XLly9XAAAAwIfS81o/AAAAALieGKQBAACggEEaAAAAChikAQAAoIBBGgAAAAoYpAEAAKCAQRoAAAAKGKQBAACggEEaAAAACvT+sL+xR48e/8vHAV3m8uXLV+V+XBNcD67W9VBVrgmuH94n4H3eJ+CDPsx14RtpAAAAKGCQBgAAgAIGaQAAAChgkAYAAIACBmkAAAAoYJAGAACAAgZpAAAAKGCQBgAAgAIGaQAAAChgkAYAAIACBmkAAAAoYJAGAACAAgZpAAAAKGCQBgAAgAIGaQAAAChgkAYAAIACBmkAAAAo0PtaP4CPgp494/9PqK+vD3uvXr3C3qNHj7DX1dWFvXfv/K//6NGjYW9vb09/JnLhwoWwX758ueh2ALg+ZO8t2fsBAFDON9IAAABQwCANAAAABQzSAAAAUMAgDQAAAAUM0gAAAFCgW23tzrZz19bWhn3UqFFhHzhwYNiHDh0a9kWLFhXdTlVV1YoVK8K+YcOGsLe0tITddm66UnYNDRkyJOznz58Pe1tbW9gvXbp0ZQ8MrpHstIbsdIfsFIdJkyaFff78+el9Z+85u3btCvuqVavCfvLkybB7/+BKZNdETU1N2IcNGxb27H1lxowZYZ8zZ07Yb7rpprBv3bo17M8++2zY9+7dG/aOjo6wAzc+30gDAABAAYM0AAAAFDBIAwAAQAGDNAAAABQwSAMAAECBbrW1O9skmW0izjYOnzlzJuzDhw8Pe7Z5MttgWVVVNWvWrLDv2LEj7NnWbuhKvXvHLxkPPvhg2D/5yU+G/cUXXwz7W2+9Ffbm5uaw2/LNtdavX7+wZ6c+3HHHHWFfsmRJ2GfOnFn8mJ5++umwZ+9dtnPTlbLN9N/4xjfC/sgjj4R93LhxYc9OPMk25Wef8WbPnh32wYMHh/3JJ58M+7Fjx8JO99O3b9+wjx49OuzZtZLdTmtra9jb29vTx3TixImwX7hwoah7n4j5RhoAAAAKGKQBAACggEEaAAAAChikAQAAoIBBGgAAAAp0q63dmdJt3qdPnw77tm3bwv7SSy+FferUqeljunjxYtizzX87d+5Mbwu6SrYVNdsy//jjj4d9/PjxYf/rX/8a9r///e9hP3r0aNizLd9nz54Ne/YakLmS7ZU2Xl7fsveD7DX5s5/9bNgfeOCBsGdbvkeMGJE+pj179oT9jTfeCHt2EgVciex1c8qUKWH/6le/GvaGhoawZ5+1du/eHfZz586FPTttIrt+Nm/eXPR46H6y5372fvCzn/0s7HPnzg37oEGDwt7W1hb2bJt3VVXV22+/HfaNGzeG/U9/+lPYt2zZEvZsy3d34RtpAAAAKGCQBgAAgAIGaQAAAChgkAYAAIACBmkAAAAo0K22dmdbczs6OsJ+5MiRsGfb+rLNdW+++WbY6+vrw15VVTVkyJCwNzU1hd1GYK6G7Hk2duzYsGebJLNt23feeWfYBw8eHPZsC/GaNWvCnl3rw4YNK7r9bNtrVeV/tuz1Ifs7dU1fG9nr+8CBA8OePWfvueeesGdbuFtaWsLe2abt9evXh/3MmTNhv3TpUnpbUKpPnz5hX7BgQdizzfTZ8/WZZ54J++9+97uwZ9dKdu1m10NjY2PYs63gdD/ZCSbTp08P+x133BH27Joovd9sy3dV5Z/P5s2bF/b58+eH/Qc/+EHYs1Miuss2b99IAwAAQAGDNAAAABQwSAMAAEABgzQAAAAUMEgDAABAAYM0AAAAFOhWx1/17h3/cfv27Rv27PiZ9vb2sGer3o8fPx72ixcvhr2qqurQoUNhz44Tyo5scYQOXWno0KFhz44Reemll8K+evXqsLe1tYV94sSJYf/0pz8d9iVLloR9wIABYc+OX3nllVfCvnz58rBXVX7NZddoJvv9jjD638reDxYuXBj2b37zm2HP3m/efvvtsGfHpmVHv1VVfhxidt/Qlerq6sJ+9913F93Oxo0bw/7rX/867AcPHgx79tqbHRmU6ezIOaiqqurZM/4eMjveMDs6bdu2bWHPjpTKrpXs+MSqqqqPf/zjRT07Li77HLZp06b0vrsD30gDAABAAYM0AAAAFDBIAwAAQAGDNAAAABQwSAMAAECBbrXaM9uS3b9//7Bn21Kz7arNzc1hz7bAfvnLXw57VeXb8RYvXhz2w4cPp7cFpbKN0TNnzgx7Q0ND2JctWxb2v/3tb2HPNt9nm4537twZ9rlz54a9trY27GvWrAn7+vXrw55t+a4qW7WvF9km32zj8A9/+MOwZ+8Ha9euDfu6devCnm0irqmpCXtnSjfHO92BK5Gd4pBtA84+C23evDnsJ0+eDHv2GlvaM64H/pvsOZJ9hslOUti+fXvYf/7zn4d969atYe/sFKCXX3457BMmTAh7NhNlG8M7OjrS++4OfCMNAAAABQzSAAAAUMAgDQAAAAUM0gAAAFDAIA0AAAAFutXW7mzLXl1dXdhnz54d9k2bNoU92wz57W9/O+z33Xdf2Kuqqs6dOxf2EydOpD8DXSXbrnrvvfeG/dSpU2HfsGFD2Nvb28OebRXOtmTv3r077NmW79bW1qJuA/eNa9CgQWH/1re+FfYRI0aE/fnnnw/773//+7A3NTWFPXuuZddEVeWbYD1vuRpGjRoV9vr6+rBnz+XsNIWbbrop7Pv27Qt7tj04++yXXSc9e8bfMXW2GZnuJXsuZJvmz58/H/bTp0+H/fjx42HPtoJ3Jvu8ld13thk8m0u6O99IAwAAQAGDNAAAABQwSAMAAEABgzQAAAAUMEgDAABAAVu7q6o6e/Zs2LPteGPHjg17tmFy8eLFYe9ss+qyZcvCfvjw4fRnoKtMmTIl7Pfcc0/Ys02V2ebJbONlr169wp5di9m28KNHjxbdDjeubANvQ0ND2CdNmhT2bHP8xo0bw97Y2Bj2bHtr9v7UmWwLcvZnhiuRPc9GjhwZ9ux1Ntsyn23EnzdvXtjHjBkT9kOHDoU9+4yX9ex0lOz325Lf/WT/5tmpPitWrAj71KlTwz5//vyw79ix40M8uv8ve8/Jerb9nph3WwAAAChgkAYAAIACBmkAAAAoYJAGAACAAgZpAAAAKGBrd5VvmBw3blzYFy1aFPZsE/Ho0aPDnm2YrKqq+uUvfxl22yHpStl23wkTJoS9X79+Yf/zn/8c9paWlrBnz+Ps8WRbY7Otk9m1yEdL9u96JRusSw0aNCjsffr0CXtzc3PYs23e2XOwK/9s2W1l9539fWfb8rPr9Gr8+/DR19raGvZsu3VNTU3YsxNPHn744bDv3r077HV1dWHPruns1Ifs89e+ffvC3t7eHnbvQ91P9rn+V7/6VdifeuqpsD/xxBNhz+aV06dPp48pe71eu3Zt2Lds2ZLeFh/kG2kAAAAoYJAGAACAAgZpAAAAKGCQBgAAgAIGaQAAACjQrbZ2ZxtLBw8eHPa5c+eGfcqUKWHftWtX2LPNwgcPHgx7VeXbLa/llltuPNm23lGjRoV98+bNYd+0aVPYs+dxtg14+PDhYZ84cWLY33zzzbC7Hq4PV+PfKXuu7dy5M+yvvvpq2LON9Q0NDWGvra0Ne7ZdtSv/LrL3if79+4d92rRpYc+2Mjc2NoY922DuerwxdXR0hD3bLJxtyc62fGfP1+x9IjtpJbudYcOGhX3GjBlh/+c//xn2bMt3ZyezOIHlxpT9u2avmUuXLg17djrQnDlzwt63b9/0MY0fPz7s2XtU9rkwu667O99IAwAAQAGDNAAAABQwSAMAAEABgzQAAAAUMEgDAABAgW61tTvbHJr10aNHF93+/v37i35/ZxsdZ8+eHfZs02y2CRauRPZ8Wrt2bdibm5vDnm2LHDBgQNi/+MUvhj3bSJlt7Yb/5uTJk2H/8Y9/HPYvfOELYc+2dk+aNCns2ekObW1tYb+S7b7ZhvGbb7457E8++WTY161bF/YXXngh7Pv27Qt7tt2Z60P2GSn7zLNnz56wZ9u5X3755bD37h1/RM22yWfX9IgRI8L+4IMPhn369Olh79kz/u7pH//4R9iPHz8e9qqqqnPnzqW/xo2ndJv3b3/727CvXr067BMmTEjvO3t9v//++4vu+8CBA+l9dGe+kQYAAIACBmkAAAAoYJAGAACAAgZpAAAAKGCQBgAAgALdamt3Jtsk+e6774Z95syZYW9paQl7tkH14sWL6WP63Oc+F/aJEyeG/amnngp7tt0Sqirfivrvf/877EeOHAl79jyrr68Pe7ZFddq0aWEfOnRo0e10ti0VqqqqLly4EPZsY/0tt9wS9iFDhoR93rx5Yc/eVw4fPlz0eKqqqv7zn/+kvxb50pe+FPZPfepTYc+25f/0pz8Nu+3c3UtTU1PYv//974e9vb29qGeb7LPff/78+bDX1taGPXsNuP3228N+6623Ft1O1uE92Ub8bKt79j5RU1OT3kc249x2221hz57nBw8eDHv2Z+gufCMNAAAABQzSAAAAUMAgDQAAAAUM0gAAAFDAIA0AAAAFbO2uqurQoUNhX7VqVdhLNx1nW1c729p95513hv3RRx8N+8CBA8O+dOnSsDc3N4e9u2/fu5716NEj7NnW+KqqqkGDBoV99+7dYT9z5kzRfWdOnDgR9mxL61133RX2uXPnhj27Fj2/ec+lS5fCnp2+8Oyzz4Z9/vz5Yc+2eWd9+PDhYd++fXvYqyrf1DpmzJiwP/LII2EfMGBA2LPrJdsCS/eSbc9+4403wp69T/TsGX+nk12jpduws9vPenbaROntZ48f/pvstTd7TmUb66sq3wBeV1cX9gULFoT9tddeK77v7sA30gAAAFDAIA0AAAAFDNIAAABQwCANAAAABQzSAAAAUMDW7qqqOjo6wv7666+HPdual200zm4/2/RYVVXVq1evsGcbXx944IGwr1y5Muyvvvpq2Eu3YfLRkW1EzTa6V1VVTZo0KezZ82Dbtm1hz57j2Xbfffv2hX39+vVhX7hwYdgnT54c9uza6mxTPlRVvoF069atYW9qagr73XffHfZs8+mRI0eKelXlz/MRI0aEPXtfyTzzzDNht42YzpS+/mbvH6WnLGT3W1tbG/Z777236Pfv378/7E6J4GrJXnuPHTuW/swLL7wQ9hkzZoT9oYceCvsvfvGLsGfXRXfhG2kAAAAoYJAGAACAAgZpAAAAKGCQBgAAgAIGaQAAAChga3eVb8E7ceJE2LNNjJ/5zGfCnm0ubmxsTB/T0aNHw37LLbeEfcuWLeltRWyTvPFk/6bZBtKqqqolS5aEPdvm/dxzz4V9zZo1Yc+2tGYb7t95552wZ9uGH3/88bD/6Ec/Kno88J5sY31ra2vYz507F/bly5eHPXs/uBLZe9fJkyfD3q9fv7BnW5M3b958ZQ+M60526kNnv9a/f/+i3tbWFvbsdTm739LHM2vWrLBnm/Uz2UkunW3Wh66Ufc5rb29Pf2bDhg1hzz6HjRo1Kuxjx44N+4EDB8LeXeYM30gDAABAAYM0AAAAFDBIAwAAQAGDNAAAABQwSAMAAEABW7s7Ubodb/LkyWHPNqiOHDkyve+ZM2eGvW/fvmE/fvx42LMtyLYX33iy52tTU1P6M++++27YP//5z4d99uzZYV+0aFHR7Wfbhnfs2BH2U6dOhX3MmDFhz7YTd+XGZG5M2XPz/PnzYR86dGjYW1pauuwxlZowYULY6+vrw55tb927d2+XPSY+GrKN1zU1NenP9O4df1TMXmezzxfZ6QudbQwvud/p06eH/Wtf+1rYhw0bFvbsM97zzz8f9mzTP1ypnj3j7zmvZBP2gAEDwp69150+fbrovrPr19ZuAAAA4AMM0gAAAFDAIA0AAAAFDNIAAABQwCANAAAABWzt7kS2eXLDhg1hzzYLZ5vxss3cVZVv2Tt69GjYs22S2e+n+zh37lz6ay+++GLYsy2tjz32WNHvr62tDXtHR0fY+/fvH/Zs23a2bTi7nWu5SZnrW7a1+8CBA2HPXvevhoULF4Y926Kabcvv7LWDj7Zsk+6QIUPCvmDBgvS2br755rA3NjaGPTstpHS7dbble9q0aWF/+OGHw3777beHvfQz3muvvRb2a3mtc33IrsfsJJ4+ffqEPfvs1NnW/a985Sthb2hoCPvBgwfDXrpdv7vwjTQAAAAUMEgDAABAAYM0AAAAFDBIAwAAQAGDNAAAABSwtfsKtLe3h33NmjVhnzdvXtizbX1VlW/bfvrpp8O+bt26sGdbKaGq8ufyqlWrwt6zZ/x/byNGjAh7tiU42549Z86csA8fPjzshw4dCnu2zRu62rV8je3Xr1/YH3roobBnjzU79cE24htPfX192O+77770Z7LPMNn7weHDh4se0969e8M+dOjQsI8fPz7s2Uby7NSHjRs3hv073/lO2E+ePBl2eE+22Tp7rR48eHDYs2srOwnle9/7XvqYHn300bA3NzeH/Q9/+EPYt23bFvbu/j7hG2kAAAAoYJAGAACAAgZpAAAAKGCQBgAAgAIGaQAAAChgkAYAAIACjr+6AhcuXAj7ihUrwv7666+HferUqel9HDt2LOybNm0Ke3aMEXQmO57q+PHjYV+9enXYJ0+eHPbZs2eHPTsu6+tf/3rYs8e5bNmysDv+ihtFnz590l974oknwj5r1qywZ8edZMdfceM5ffp02LMjNzv7mYaGhrBPmTIl7P379w979lmoo6Mj7KdOnQr7rl27wv6Xv/wl7CtXrgz7/v37w+44Ud6THXNVV1cX9hkzZoT91ltvDfugQYPCftddd4X9/vvvD3tVVVVra2vYX3nllbBnn6uy14HuzjfSAAAAUMAgDQAAAAUM0gAAAFDAIA0AAAAFDNIAAABQwNbuLpRtzt6+fXvYd+zYUXwfly5dKv4ZKJVtJz1w4EDYs62QPXvG/1c3bty4sA8YMCDs//rXv8L+xz/+MeyuE24UM2fOTH/tu9/9btizTd/Zdu7GxsbyB8ZHWnbSQba5/Sc/+Ul6W9nr7MSJE8N+2223hX3kyJFhz05CybZnZ+9D77zzTtj37NkT9paWlrDbzs1/069fv7Bnr9ePPfZY2LNt3sOHDw/72bNnw97ZPJGdKLR8+fKwZ6e2ZK8p3Z1vpAEAAKCAQRoAAAAKGKQBAACggEEaAAAAChikAQAAoICt3VdBtunOBjyuN6WbYLNt29mW1rq6urCvXLky7Dt37gy7a4sbxVtvvZX+2m9+85uwf+ITnwj70qVLw27LffeR/VsfO3Ys/Zns1zZu3Bj2Hj16lD+wQOnruNd9ulp28kh9fX3YFy9eHPY5c+aEPdtkn11z69atC/tzzz0X9qqqqk2bNoU9O2nIdVTGN9IAAABQwCANAAAABQzSAAAAUMAgDQAAAAUM0gAAAFCgx+UPuZ6tq7Ywwv/a1do46Jq4cr169Qr7wIEDw97W1hb28+fPd9ljulFdzQ2cromrK/v7zrrt3O/zPgHv8z5RJvsz1NTUhP1jH/tY2FtbW8Pe1NQU9o6OjrB7bf/f+DDXhW+kAQAAoIBBGgAAAAoYpAEAAKCAQRoAAAAKGKQBAACggK3d3HBsY4X32cYKH+R9At7nfQI+yNZuAAAA6GIGaQAAAChgkAYAAIACBmkAAAAoYJAGAACAAgZpAAAAKGCQBgAAgAIGaQAAAChgkAYAAIACBmkAAAAoYJAGAACAAj0uX758+Vo/CAAAALhe+EYaAAAAChikAQAAoIBBGgAAAAoYpAEAAKCAQRoAAAAKGKQBAACggEEaAAAAChikAQAAoIBBGgAAAAr8HxY5kRxcg7kXAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generated = generated.cpu()\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    axs[i].imshow(generated[i].squeeze(), cmap='gray')\n",
        "    axs[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
